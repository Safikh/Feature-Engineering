{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/biography.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marie Curie was a Polish-born physicist and chemist and one of the most famous scientists of her time.',\n",
       " 'Together with her husband Pierre, she was awarded the Nobel Prize in 1903, and she went on to win another in 1911.',\n",
       " 'Marie Sklodowska was born in Warsaw on 7 November 1867, the daughter of a teacher.',\n",
       " 'In 1891, she went to Paris to study physics and mathematics at the Sorbonne where she met Pierre Curie, professor of the School of Physics.',\n",
       " 'They were married in 1895.',\n",
       " 'The Curies worked together investigating radioactivity, building on the work of the German physicist Roentgen and the French physicist Becquerel.',\n",
       " 'In July 1898, the Curies announced the discovery of a new chemical element, polonium.',\n",
       " 'At the end of the year, they announced the discovery of another, radium.',\n",
       " 'The Curies, along with Becquerel, were awarded the Nobel Prize for Physics in 1903.',\n",
       " \"Pierre's life was cut short in 1906 when he was knocked down and killed by a carriage.\",\n",
       " 'Marie took over his teaching post, becoming the first woman to teach at the Sorbonne, and devoted herself to continuing the work that they had begun together.',\n",
       " 'She received a second Nobel Prize, for Chemistry, in 1911.',\n",
       " \"The Curie's research was crucial in the development of x-rays in surgery.\",\n",
       " 'During World War One Curie helped to equip ambulances with x-ray equipment, which she herself drove to the front lines.',\n",
       " 'The International Red Cross made her head of its radiological service and she held training courses for medical orderlies and doctors in the new techniques.',\n",
       " 'Despite her success, Marie continued to face great opposition from male scientists in France, and she never received significant financial benefits from her work.',\n",
       " 'By the late 1920s her health was beginning to deteriorate.',\n",
       " 'She died on 4 July 1934 from leukaemia, caused by exposure to high-energy radiation from her research.',\n",
       " \"The Curies' eldest daughter Irene was herself a scientist and winner of the Nobel Prize for Chemistry.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.,  -2.,   7.,   5.,   0., -17.,   0.,  -2.],\n",
       "       [ 22.,   0.,   5.,   1.,  -6., -22.,   0.,   8.],\n",
       "       [ 17.,   1.,   8.,   4.,  -2.,  -9.,   0.,   1.],\n",
       "       [ 26.,  -1.,   6.,   3.,  -3., -20.,   1.,   3.],\n",
       "       [  4.,   2.,   1.,   1.,   1.,  -6.,   1.,   4.],\n",
       "       [ 21.,   3.,  -1.,  -3.,  -6., -25.,   0.,   8.],\n",
       "       [ 17.,  -4.,   4.,   2.,   0., -10.,   1.,   9.],\n",
       "       [ 14.,   2.,   4.,   2.,  -4.,  -8.,   4.,   8.],\n",
       "       [ 14.,  -2.,   2.,   2.,   2., -14.,   0.,   7.],\n",
       "       [ 18.,  -3.,   5.,   6.,  -2., -11.,   0.,   1.],\n",
       "       [ 28.,   1.,   4.,  -2.,  -8., -22.,   0.,  11.],\n",
       "       [  9.,   1.,   1.,   2.,  -1., -12.,   1.,   5.],\n",
       "       [ 14.,   2.,   3.,   2.,   0., -10.,   0.,   4.],\n",
       "       [ 20.,   3.,   3.,   3.,   1., -17.,   0.,  10.],\n",
       "       [ 25.,   5.,   9.,  14.,  -2., -21.,   0.,   8.],\n",
       "       [ 25.,   0.,   7.,   4.,  -8., -20.,  -2.,   8.],\n",
       "       [  9.,   2.,   2.,  -3.,  -1., -10.,   0.,   7.],\n",
       "       [ 18.,   6.,   4.,  10.,   2., -13.,   2.,   9.],\n",
       "       [ 18.,   3.,   4.,   3.,  -1., -16.,   0.,   7.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher = FeatureHasher(n_features=8, input_type='string')\n",
    "hashed_sents = hasher.fit_transform(sentences)\n",
    "hashed_sents.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.,  -1.,   6.,   2.,  -5., -10.,   0.,   8.,  17.,  -1.,   1.,\n",
       "          3.,   5.,  -7.,   0., -10.],\n",
       "       [  1.,   3.,   6.,   1.,  -7.,  -6.,   0.,  13.,  21.,  -3.,  -1.,\n",
       "          0.,   1., -16.,   0.,  -5.],\n",
       "       [  1.,   2.,   8.,   4.,  -3.,  -2.,   0.,   7.,  16.,  -1.,   0.,\n",
       "          0.,   1.,  -7.,   0.,  -6.],\n",
       "       [  2.,   3.,   5.,   0.,  -5.,  -7.,   1.,  14.,  24.,  -4.,   1.,\n",
       "          3.,   2., -13.,   0., -11.],\n",
       "       [  0.,   3.,   1.,   1.,   0.,  -2.,   1.,   4.,   4.,  -1.,   0.,\n",
       "          0.,   1.,  -4.,   0.,   0.],\n",
       "       [  3.,   2.,   5.,  -2.,  -9., -14.,   0.,  16.,  18.,   1.,  -6.,\n",
       "         -1.,   3., -11.,   0.,  -8.],\n",
       "       [  4.,  -2.,   3.,   2.,  -7.,  -5.,   1.,  11.,  13.,  -2.,   1.,\n",
       "          0.,   7.,  -5.,   0.,  -2.],\n",
       "       [  2.,   2.,   4.,   1.,  -5.,  -2.,   3.,   9.,  12.,   0.,   0.,\n",
       "          1.,   1.,  -6.,   1.,  -1.],\n",
       "       [  2.,   1.,   3.,   1.,  -1.,  -6.,   0.,  11.,  12.,  -3.,  -1.,\n",
       "          1.,   3.,  -8.,   0.,  -4.],\n",
       "       [  2.,  -2.,   6.,   5.,  -5.,  -5.,   1.,   8.,  16.,  -1.,  -1.,\n",
       "          1.,   3.,  -6.,  -1.,  -7.],\n",
       "       [  2.,   1.,   8.,  -2., -10.,  -7.,   0.,  17.,  26.,   0.,  -4.,\n",
       "          0.,   2., -15.,   0.,  -6.],\n",
       "       [  0.,   3.,   1.,   2.,  -2.,  -5.,   1.,   8.,   9.,  -2.,   0.,\n",
       "          0.,   1.,  -7.,   0.,  -3.],\n",
       "       [  3.,   2.,   4.,   1.,  -3.,  -5.,   1.,   9.,  11.,   0.,  -1.,\n",
       "          1.,   3.,  -5.,  -1.,  -5.],\n",
       "       [  3.,   3.,   4.,   1.,  -6.,  -8.,  -1.,  14.,  17.,   0.,  -1.,\n",
       "          2.,   7.,  -9.,   1.,  -4.],\n",
       "       [  2.,   5.,  10.,  13., -10., -12.,   0.,  18.,  23.,   0.,  -1.,\n",
       "          1.,   8.,  -9.,   0., -10.],\n",
       "       [  2.,   0.,   9.,   0., -13., -15.,  -2.,  20.,  23.,   0.,  -2.,\n",
       "          4.,   5.,  -5.,   0., -12.],\n",
       "       [  0.,   2.,   4.,  -3.,  -3.,  -3.,   0.,   9.,   9.,   0.,  -2.,\n",
       "          0.,   2.,  -7.,   0.,  -2.],\n",
       "       [  4.,   6.,   6.,   7.,  -2.,  -5.,   2.,  13.,  14.,   0.,  -2.,\n",
       "          3.,   4.,  -8.,   0.,  -4.],\n",
       "       [  2.,   5.,   4.,   0.,  -5.,  -8.,   1.,  15.,  16.,  -2.,   0.,\n",
       "          3.,   4.,  -8.,  -1.,  -8.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher = FeatureHasher(n_features=16, input_type='string')\n",
    "hashed_sents = hasher.fit_transform(sentences)\n",
    "hashed_sents.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vector = count_vectorizer.fit_transform(sentences)\n",
    "count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = count_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marie',\n",
       " 'curie',\n",
       " 'was',\n",
       " 'polish',\n",
       " 'born',\n",
       " 'physicist',\n",
       " 'and',\n",
       " 'chemist',\n",
       " 'and',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'famous',\n",
       " 'scientists',\n",
       " 'of',\n",
       " 'her',\n",
       " 'time']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = analyzer(sentences[0])\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_list = []\n",
    "\n",
    "for i, text in enumerate(sentences):\n",
    "    tokens = analyzer(text)\n",
    "    word_frequency = {}\n",
    "    for token in tokens:\n",
    "        word_idx = count_vectorizer.vocabulary_.get(token)\n",
    "        word_frequency[token] = count_vector[i, word_idx]\n",
    "    frequency_list.append(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'marie': 1,\n",
       "  'curie': 1,\n",
       "  'was': 1,\n",
       "  'polish': 1,\n",
       "  'born': 1,\n",
       "  'physicist': 1,\n",
       "  'and': 2,\n",
       "  'chemist': 1,\n",
       "  'one': 1,\n",
       "  'of': 2,\n",
       "  'the': 1,\n",
       "  'most': 1,\n",
       "  'famous': 1,\n",
       "  'scientists': 1,\n",
       "  'her': 1,\n",
       "  'time': 1},\n",
       " {'together': 1,\n",
       "  'with': 1,\n",
       "  'her': 1,\n",
       "  'husband': 1,\n",
       "  'pierre': 1,\n",
       "  'she': 2,\n",
       "  'was': 1,\n",
       "  'awarded': 1,\n",
       "  'the': 1,\n",
       "  'nobel': 1,\n",
       "  'prize': 1,\n",
       "  'in': 2,\n",
       "  '1903': 1,\n",
       "  'and': 1,\n",
       "  'went': 1,\n",
       "  'on': 1,\n",
       "  'to': 1,\n",
       "  'win': 1,\n",
       "  'another': 1,\n",
       "  '1911': 1},\n",
       " {'marie': 1,\n",
       "  'sklodowska': 1,\n",
       "  'was': 1,\n",
       "  'born': 1,\n",
       "  'in': 1,\n",
       "  'warsaw': 1,\n",
       "  'on': 1,\n",
       "  'november': 1,\n",
       "  '1867': 1,\n",
       "  'the': 1,\n",
       "  'daughter': 1,\n",
       "  'of': 1,\n",
       "  'teacher': 1},\n",
       " {'in': 1,\n",
       "  '1891': 1,\n",
       "  'she': 2,\n",
       "  'went': 1,\n",
       "  'to': 2,\n",
       "  'paris': 1,\n",
       "  'study': 1,\n",
       "  'physics': 2,\n",
       "  'and': 1,\n",
       "  'mathematics': 1,\n",
       "  'at': 1,\n",
       "  'the': 2,\n",
       "  'sorbonne': 1,\n",
       "  'where': 1,\n",
       "  'met': 1,\n",
       "  'pierre': 1,\n",
       "  'curie': 1,\n",
       "  'professor': 1,\n",
       "  'of': 2,\n",
       "  'school': 1},\n",
       " {'they': 1, 'were': 1, 'married': 1, 'in': 1, '1895': 1},\n",
       " {'the': 4,\n",
       "  'curies': 1,\n",
       "  'worked': 1,\n",
       "  'together': 1,\n",
       "  'investigating': 1,\n",
       "  'radioactivity': 1,\n",
       "  'building': 1,\n",
       "  'on': 1,\n",
       "  'work': 1,\n",
       "  'of': 1,\n",
       "  'german': 1,\n",
       "  'physicist': 2,\n",
       "  'roentgen': 1,\n",
       "  'and': 1,\n",
       "  'french': 1,\n",
       "  'becquerel': 1},\n",
       " {'in': 1,\n",
       "  'july': 1,\n",
       "  '1898': 1,\n",
       "  'the': 2,\n",
       "  'curies': 1,\n",
       "  'announced': 1,\n",
       "  'discovery': 1,\n",
       "  'of': 1,\n",
       "  'new': 1,\n",
       "  'chemical': 1,\n",
       "  'element': 1,\n",
       "  'polonium': 1},\n",
       " {'at': 1,\n",
       "  'the': 3,\n",
       "  'end': 1,\n",
       "  'of': 2,\n",
       "  'year': 1,\n",
       "  'they': 1,\n",
       "  'announced': 1,\n",
       "  'discovery': 1,\n",
       "  'another': 1,\n",
       "  'radium': 1},\n",
       " {'the': 2,\n",
       "  'curies': 1,\n",
       "  'along': 1,\n",
       "  'with': 1,\n",
       "  'becquerel': 1,\n",
       "  'were': 1,\n",
       "  'awarded': 1,\n",
       "  'nobel': 1,\n",
       "  'prize': 1,\n",
       "  'for': 1,\n",
       "  'physics': 1,\n",
       "  'in': 1,\n",
       "  '1903': 1},\n",
       " {'pierre': 1,\n",
       "  'life': 1,\n",
       "  'was': 2,\n",
       "  'cut': 1,\n",
       "  'short': 1,\n",
       "  'in': 1,\n",
       "  '1906': 1,\n",
       "  'when': 1,\n",
       "  'he': 1,\n",
       "  'knocked': 1,\n",
       "  'down': 1,\n",
       "  'and': 1,\n",
       "  'killed': 1,\n",
       "  'by': 1,\n",
       "  'carriage': 1},\n",
       " {'marie': 1,\n",
       "  'took': 1,\n",
       "  'over': 1,\n",
       "  'his': 1,\n",
       "  'teaching': 1,\n",
       "  'post': 1,\n",
       "  'becoming': 1,\n",
       "  'the': 3,\n",
       "  'first': 1,\n",
       "  'woman': 1,\n",
       "  'to': 2,\n",
       "  'teach': 1,\n",
       "  'at': 1,\n",
       "  'sorbonne': 1,\n",
       "  'and': 1,\n",
       "  'devoted': 1,\n",
       "  'herself': 1,\n",
       "  'continuing': 1,\n",
       "  'work': 1,\n",
       "  'that': 1,\n",
       "  'they': 1,\n",
       "  'had': 1,\n",
       "  'begun': 1,\n",
       "  'together': 1},\n",
       " {'she': 1,\n",
       "  'received': 1,\n",
       "  'second': 1,\n",
       "  'nobel': 1,\n",
       "  'prize': 1,\n",
       "  'for': 1,\n",
       "  'chemistry': 1,\n",
       "  'in': 1,\n",
       "  '1911': 1},\n",
       " {'the': 2,\n",
       "  'curie': 1,\n",
       "  'research': 1,\n",
       "  'was': 1,\n",
       "  'crucial': 1,\n",
       "  'in': 2,\n",
       "  'development': 1,\n",
       "  'of': 1,\n",
       "  'rays': 1,\n",
       "  'surgery': 1},\n",
       " {'during': 1,\n",
       "  'world': 1,\n",
       "  'war': 1,\n",
       "  'one': 1,\n",
       "  'curie': 1,\n",
       "  'helped': 1,\n",
       "  'to': 2,\n",
       "  'equip': 1,\n",
       "  'ambulances': 1,\n",
       "  'with': 1,\n",
       "  'ray': 1,\n",
       "  'equipment': 1,\n",
       "  'which': 1,\n",
       "  'she': 1,\n",
       "  'herself': 1,\n",
       "  'drove': 1,\n",
       "  'the': 1,\n",
       "  'front': 1,\n",
       "  'lines': 1},\n",
       " {'the': 2,\n",
       "  'international': 1,\n",
       "  'red': 1,\n",
       "  'cross': 1,\n",
       "  'made': 1,\n",
       "  'her': 1,\n",
       "  'head': 1,\n",
       "  'of': 1,\n",
       "  'its': 1,\n",
       "  'radiological': 1,\n",
       "  'service': 1,\n",
       "  'and': 2,\n",
       "  'she': 1,\n",
       "  'held': 1,\n",
       "  'training': 1,\n",
       "  'courses': 1,\n",
       "  'for': 1,\n",
       "  'medical': 1,\n",
       "  'orderlies': 1,\n",
       "  'doctors': 1,\n",
       "  'in': 1,\n",
       "  'new': 1,\n",
       "  'techniques': 1},\n",
       " {'despite': 1,\n",
       "  'her': 2,\n",
       "  'success': 1,\n",
       "  'marie': 1,\n",
       "  'continued': 1,\n",
       "  'to': 1,\n",
       "  'face': 1,\n",
       "  'great': 1,\n",
       "  'opposition': 1,\n",
       "  'from': 2,\n",
       "  'male': 1,\n",
       "  'scientists': 1,\n",
       "  'in': 1,\n",
       "  'france': 1,\n",
       "  'and': 1,\n",
       "  'she': 1,\n",
       "  'never': 1,\n",
       "  'received': 1,\n",
       "  'significant': 1,\n",
       "  'financial': 1,\n",
       "  'benefits': 1,\n",
       "  'work': 1},\n",
       " {'by': 1,\n",
       "  'the': 1,\n",
       "  'late': 1,\n",
       "  '1920s': 1,\n",
       "  'her': 1,\n",
       "  'health': 1,\n",
       "  'was': 1,\n",
       "  'beginning': 1,\n",
       "  'to': 1,\n",
       "  'deteriorate': 1},\n",
       " {'she': 1,\n",
       "  'died': 1,\n",
       "  'on': 1,\n",
       "  'july': 1,\n",
       "  '1934': 1,\n",
       "  'from': 2,\n",
       "  'leukaemia': 1,\n",
       "  'caused': 1,\n",
       "  'by': 1,\n",
       "  'exposure': 1,\n",
       "  'to': 1,\n",
       "  'high': 1,\n",
       "  'energy': 1,\n",
       "  'radiation': 1,\n",
       "  'her': 1,\n",
       "  'research': 1},\n",
       " {'the': 2,\n",
       "  'curies': 1,\n",
       "  'eldest': 1,\n",
       "  'daughter': 1,\n",
       "  'irene': 1,\n",
       "  'was': 1,\n",
       "  'herself': 1,\n",
       "  'scientist': 1,\n",
       "  'and': 1,\n",
       "  'winner': 1,\n",
       "  'of': 1,\n",
       "  'nobel': 1,\n",
       "  'prize': 1,\n",
       "  'for': 1,\n",
       "  'chemistry': 1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher = FeatureHasher(n_features=8, input_type='string')\n",
    "hashed_sents = hasher.fit_transform(frequency_list)\n",
    "hashed_sents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1., -1.,  0., -2., -1.,  0.],\n",
       "       [ 0.,  3., -1.,  0., -4.,  2., -1., -1.],\n",
       "       [ 0.,  2.,  1., -1.,  2.,  2.,  0., -1.],\n",
       "       [ 0.,  2.,  0.,  3., -1., -1.,  0.,  1.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.,  1.,  1.,  1.],\n",
       "       [ 1.,  2.,  1.,  0.,  1.,  0., -1., -2.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  0., -1.,  1.],\n",
       "       [ 2.,  0., -1.,  0.,  0.,  0., -1.,  0.],\n",
       "       [-1.,  3.,  0., -2., -1., -2.,  0.,  0.],\n",
       "       [ 1.,  3., -1.,  3., -1., -1.,  1.,  0.],\n",
       "       [-1.,  0.,  1.,  2.,  0., -2., -2., -2.],\n",
       "       [ 0.,  2., -1.,  0., -2.,  1.,  1.,  0.],\n",
       "       [ 0.,  3.,  0.,  0.,  1.,  0., -3., -1.],\n",
       "       [-1.,  1.,  0.,  1.,  0., -1., -3.,  2.],\n",
       "       [ 1.,  1.,  1., -2.,  0.,  0., -2.,  0.],\n",
       "       [ 0.,  3., -1., -3.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  1.,  1., -1.,  1., -1.,  0.],\n",
       "       [ 0.,  0., -2.,  2.,  2.,  0.,  1., -3.],\n",
       "       [ 0.,  2.,  0.,  0., -2.,  1., -2.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_sents.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer \n",
    "# Hashing Vectorizer = Count Vectorizer + Feature Hasher (Results need not be an exact match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=8, norm=None) # l1 and l2 norm can also be used to normalize each vector\n",
    "feature_vector = vectorizer.fit_transform(sentences)\n",
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1., -1.,  1., -3., -1.,  0.],\n",
       "       [ 0.,  4., -1.,  0., -4.,  3., -1., -1.],\n",
       "       [ 0.,  2.,  1., -1.,  2.,  2.,  0., -1.],\n",
       "       [ 0.,  2.,  0.,  4.,  0., -1., -1.,  1.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.,  1.,  0., -4., -2.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  0., -2.,  1.],\n",
       "       [ 2.,  0., -1.,  0.,  1.,  0., -3.,  0.],\n",
       "       [-1.,  3.,  0., -2., -1., -2., -1.,  0.],\n",
       "       [ 1.,  4., -1.,  3., -1., -1.,  1.,  0.],\n",
       "       [-1.,  0.,  1.,  3.,  0., -2., -4., -2.],\n",
       "       [ 0.,  2., -1.,  0., -2.,  1.,  1.,  0.],\n",
       "       [ 0.,  4.,  0.,  0.,  1.,  0., -4., -1.],\n",
       "       [-1.,  1.,  0.,  2.,  0., -1., -3.,  2.],\n",
       "       [ 1.,  1.,  1., -2.,  0., -1., -3.,  0.],\n",
       "       [ 0.,  3., -1., -3., -2.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  1.,  1., -1.,  1., -1.,  0.],\n",
       "       [ 0.,  0., -2.,  2.,  1.,  0.,  1., -3.],\n",
       "       [ 0.,  2.,  0.,  0., -2.,  1., -3.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality-Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary hashing performs dimensionality reduction but does not keep similar data points together. Locality-sensitive hashing addresses this problem. However, this maximizes hash collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Corpus --> Shingling --> Min-hashing --> Locality-sensitive Hashing --> Group similar documents with Jaccard Index>t\n",
    "\n",
    "Shingling is the process of generating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "from nltk import word_tokenize, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = [\"A bird in the hand is worth two in the bush\",\n",
    "                \"Good things come to those who wait\",\n",
    "                \"There are other fish in the sea\",\n",
    "                \"The ball is in your court\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'bird', 'in', 'the', 'hand', 'is', 'worth', 'two', 'in', 'the', 'bush'],\n",
       " ['Good', 'things', 'come', 'to', 'those', 'who', 'wait'],\n",
       " ['There', 'are', 'other', 'fish', 'in', 'the', 'sea'],\n",
       " ['The', 'ball', 'is', 'in', 'your', 'court']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token_array = [word_tokenize(text) for text in text_array]\n",
    "word_token_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('A', 'bird', 'in')\n",
      "0 ('bird', 'in', 'the')\n",
      "0 ('in', 'the', 'hand')\n",
      "0 ('the', 'hand', 'is')\n",
      "0 ('hand', 'is', 'worth')\n",
      "0 ('is', 'worth', 'two')\n",
      "0 ('worth', 'two', 'in')\n",
      "0 ('two', 'in', 'the')\n",
      "0 ('in', 'the', 'bush')\n",
      "1 ('Good', 'things', 'come')\n",
      "1 ('things', 'come', 'to')\n",
      "1 ('come', 'to', 'those')\n",
      "1 ('to', 'those', 'who')\n",
      "1 ('those', 'who', 'wait')\n",
      "2 ('There', 'are', 'other')\n",
      "2 ('are', 'other', 'fish')\n",
      "2 ('other', 'fish', 'in')\n",
      "2 ('fish', 'in', 'the')\n",
      "2 ('in', 'the', 'sea')\n",
      "3 ('The', 'ball', 'is')\n",
      "3 ('ball', 'is', 'in')\n",
      "3 ('is', 'in', 'your')\n",
      "3 ('in', 'your', 'court')\n"
     ]
    }
   ],
   "source": [
    "# Shingling\n",
    "for i, t in enumerate(word_token_array):\n",
    "    for N_gram in ngrams(t, 3):\n",
    "        print(i, N_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hash_lsh = MinHashLSH(threshold=0.5, num_perm=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hashes = {}\n",
    "\n",
    "for i, t in enumerate(text_array):\n",
    "    min_hash = MinHash(num_perm=128)\n",
    "\n",
    "    for n_gram in ngrams(t, 3):\n",
    "        min_hash.update(''.join(n_gram).encode('utf8'))\n",
    "\n",
    "    min_hash_lsh.insert(i, min_hash)\n",
    "    min_hashes[i] = min_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <datasketch.minhash.MinHash at 0x1b607d56688>,\n",
       " 1: <datasketch.minhash.MinHash at 0x1b607d56f48>,\n",
       " 2: <datasketch.minhash.MinHash at 0x1b607d56c48>,\n",
       " 3: <datasketch.minhash.MinHash at 0x1b607d56848>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is similar to [0]\n",
      "1 is similar to [1]\n",
      "2 is similar to [2]\n",
      "3 is similar to [3]\n"
     ]
    }
   ],
   "source": [
    "for i in min_hashes.keys():\n",
    "    result = min_hash_lsh.query(min_hashes[i])\n",
    "    print(f'{i} is similar to {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = [\"A bird in the hand is worth two in the bush\",\n",
    "                \"A bird in the hand is worth three in the bushes\",\n",
    "                \"Good things come to those who wait\",\n",
    "                \"Good tpings cxme to those who wait long\",\n",
    "                \"There are other fish in the sea\",\n",
    "                \"The ball is in your court\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hash_lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "min_hashes = {}\n",
    "for i, t in enumerate(text_array):\n",
    "    min_hash = MinHash(num_perm=128)\n",
    "\n",
    "    for n_gram in ngrams(t, 3):\n",
    "        min_hash.update(''.join(n_gram).encode('utf-8'))\n",
    "    \n",
    "    min_hash_lsh.insert(i, min_hash)\n",
    "    min_hashes[i] = min_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A bird in the hand is worth two in the bush\"\n",
      "\t is similar to \n",
      "['A bird in the hand is worth two in the bush', 'A bird in the hand is worth three in the bushes']\n",
      "\"A bird in the hand is worth three in the bushes\"\n",
      "\t is similar to \n",
      "['A bird in the hand is worth two in the bush', 'A bird in the hand is worth three in the bushes']\n",
      "\"Good things come to those who wait\"\n",
      "\t is similar to \n",
      "['Good things come to those who wait', 'Good tpings cxme to those who wait long']\n",
      "\"Good tpings cxme to those who wait long\"\n",
      "\t is similar to \n",
      "['Good things come to those who wait', 'Good tpings cxme to those who wait long']\n",
      "\"There are other fish in the sea\"\n",
      "\t is similar to \n",
      "['There are other fish in the sea']\n",
      "\"The ball is in your court\"\n",
      "\t is similar to \n",
      "['The ball is in your court']\n"
     ]
    }
   ],
   "source": [
    "for i in min_hashes.keys():\n",
    "    result = min_hash_lsh.query(min_hashes[i])\n",
    "    print(f'\"{text_array[i]}\"\\n\\t is similar to \\n{[text_array[j] for j in result]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5a8803176ba603454a120d2af569b11c3f41e6e81f2a81589e795246afd0d6d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
