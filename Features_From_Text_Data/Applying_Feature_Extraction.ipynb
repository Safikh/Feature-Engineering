{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_df = pd.read_csv('data/dbpedia_csv/train.csv',\n",
    "                            skiprows=1, names=['Label', 'Name', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Name</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Unsigned Guide</td>\n",
       "      <td>The Unsigned Guide is an online contacts dire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                               Name  \\\n",
       "0      1                     Schwan-Stabilo   \n",
       "1      1                         Q-workshop   \n",
       "2      1  Marvell Software Solutions Israel   \n",
       "3      1        Bergan Mercy Medical Center   \n",
       "4      1                 The Unsigned Guide   \n",
       "\n",
       "                                                Text  \n",
       "0   Schwan-STABILO is a German maker of pens for ...  \n",
       "1   Q-workshop is a Polish company located in Poz...  \n",
       "2   Marvell Software Solutions Israel known as RA...  \n",
       "3   Bergan Mercy Medical Center is a hospital loc...  \n",
       "4   The Unsigned Guide is an online contacts dire...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(559999, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     40000\n",
       "3     40000\n",
       "4     40000\n",
       "5     40000\n",
       "6     40000\n",
       "7     40000\n",
       "8     40000\n",
       "9     40000\n",
       "10    40000\n",
       "11    40000\n",
       "12    40000\n",
       "13    40000\n",
       "14    40000\n",
       "1     39999\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_df = dbpedia_df.sample(10000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     762\n",
       "14    738\n",
       "5     726\n",
       "2     724\n",
       "3     721\n",
       "6     721\n",
       "8     721\n",
       "4     714\n",
       "10    711\n",
       "9     709\n",
       "11    707\n",
       "12    695\n",
       "13    676\n",
       "1     675\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dbpedia_df.Text\n",
    "y = dbpedia_df.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_classification(y_test, y_pred):\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred, normalize=True)\n",
    "    num_acc = accuracy_score(y_test, y_pred, normalize=False)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f'Length of test data: {len(y_test)}')\n",
    "    print(f'Accuracy Count: {num_acc}')\n",
    "    print(f'Accuracy Score: {acc}')\n",
    "    print(f'Precision Score: {prec}')\n",
    "    print(f'Recall Score: {rec}')\n",
    "    print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "HashingVectorizer\n",
      "**************************************************\n",
      "(10000, 1024)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1134\n",
      "Accuracy Score: 0.567\n",
      "Precision Score: 0.5831351495696112\n",
      "Recall Score: 0.567\n",
      "F1 Score: 0.5702086443598284\n"
     ]
    }
   ],
   "source": [
    "print('*' * 50)\n",
    "print('HashingVectorizer')\n",
    "print('*' * 50)\n",
    "vectorizer = HashingVectorizer(n_features=2**10, norm='l2')\n",
    "feature_vector = vectorizer.fit_transform(X)\n",
    "print(feature_vector.shape)\n",
    "X_dense = np.asarray(feature_vector.todense())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "analyzer = HashingVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_vectorizer = HashingVectorizer(analyzer=stemmed_words, n_features=2**10, norm='l2')\n",
    "feature_vector = stem_vectorizer.fit_transform(X)\n",
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Stemmed HashingVectorizer\n",
      "**************************************************\n",
      "(8000, 1024) (2000, 1024) (8000,) (2000,)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1134\n",
      "Accuracy Score: 0.567\n",
      "Precision Score: 0.5831351495696112\n",
      "Recall Score: 0.567\n",
      "F1 Score: 0.5702086443598284\n"
     ]
    }
   ],
   "source": [
    "print('*' * 50)\n",
    "print('Stemmed HashingVectorizer')\n",
    "print('*' * 50)\n",
    "X_dense = np.asarray(feature_vector.todense())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "CountVectorizer\n",
      "**************************************************\n",
      "Shape of feature vector: (10000, 2000)\n",
      "(8000, 2000) (2000, 2000) (8000,) (2000,)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1465\n",
      "Accuracy Score: 0.7325\n",
      "Precision Score: 0.7328213367424521\n",
      "Recall Score: 0.7325\n",
      "F1 Score: 0.72987350840779\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print('*' * 50)\n",
    "print('CountVectorizer')\n",
    "print('*' * 50)\n",
    "count_vectorizer = CountVectorizer(max_features=2000)\n",
    "feature_vector = count_vectorizer.fit_transform(X)\n",
    "print(f'Shape of feature vector: {feature_vector.shape}')\n",
    "X_dense = feature_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "CountVectorizer with Stop Words Removed\n",
      "**************************************************\n",
      "Shape of feature vector: (10000, 2000)\n",
      "(8000, 2000) (2000, 2000) (8000,) (2000,)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1466\n",
      "Accuracy Score: 0.733\n",
      "Precision Score: 0.7309370930227532\n",
      "Recall Score: 0.733\n",
      "F1 Score: 0.7300280347618526\n"
     ]
    }
   ],
   "source": [
    "print('*' * 50)\n",
    "print('CountVectorizer with Stop Words Removed')\n",
    "print('*' * 50)\n",
    "count_vectorizer = CountVectorizer(max_features=2000, stop_words='english')\n",
    "feature_vector = count_vectorizer.fit_transform(X)\n",
    "print(f'Shape of feature vector: {feature_vector.shape}')\n",
    "X_dense = feature_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "CountVectorizer with Frequency Threshold\n",
      "**************************************************\n",
      "Number of tokens: 508124\n",
      "Number of frequent words: 503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Safiuddin\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['st'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature vector: (10000, 2000)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1306\n",
      "Accuracy Score: 0.653\n",
      "Precision Score: 0.6540085429885271\n",
      "Recall Score: 0.653\n",
      "F1 Score: 0.6454378335760009\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "print('*' * 50)\n",
    "print('CountVectorizer with Frequency Threshold')\n",
    "print('*' * 50)\n",
    "tokens = word_tokenize('\\n'.join(X.values))\n",
    "print(f'Number of tokens: {len(tokens)}')\n",
    "fdist = FreqDist(tokens)\n",
    "freq_words = []\n",
    "for word, freq in fdist.items():\n",
    "    if freq >= 100:\n",
    "        freq_words.append(word.lower())\n",
    "print(f'Number of frequent words: {len(freq_words)}')\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(freq_words)\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=2000, stop_words=stop_words)\n",
    "feature_vector = count_vectorizer.fit_transform(X)\n",
    "print(f'Shape of feature vector: {feature_vector.shape}')\n",
    "X_dense = feature_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "TfidfVectorizer with Stop Words Removed\n",
      "**************************************************\n",
      "Shape of feature vector: (10000, 2000)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1464\n",
      "Accuracy Score: 0.732\n",
      "Precision Score: 0.7369835482836727\n",
      "Recall Score: 0.732\n",
      "F1 Score: 0.7328047604979636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print('*' * 50)\n",
    "print('TfidfVectorizer with Stop Words Removed')\n",
    "print('*' * 50)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
    "feature_vector = tfidf_vectorizer.fit_transform(X)\n",
    "print(f'Shape of feature vector: {feature_vector.shape}')\n",
    "X_dense = feature_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Bag of n-grams vectorizer\n",
      "**************************************************\n",
      "Shape of feature vector: (10000, 2000)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1584\n",
      "Accuracy Score: 0.792\n",
      "Precision Score: 0.7951879407472687\n",
      "Recall Score: 0.792\n",
      "F1 Score: 0.7861216325000008\n"
     ]
    }
   ],
   "source": [
    "print('*' * 50)\n",
    "print('Bag of n-grams vectorizer')\n",
    "print('*' * 50)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=2000)\n",
    "feature_vector = count_vectorizer.fit_transform(X)\n",
    "print(f'Shape of feature vector: {feature_vector.shape}')\n",
    "X_dense = feature_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Bag of n-grams vectorizer with Stop Words Removed\n",
      "**************************************************\n",
      "Shape of feature vector: (10000, 2000)\n",
      "Length of test data: 2000\n",
      "Accuracy Count: 1414\n",
      "Accuracy Score: 0.707\n",
      "Precision Score: 0.7910931651237748\n",
      "Recall Score: 0.707\n",
      "F1 Score: 0.7170837940139252\n"
     ]
    }
   ],
   "source": [
    "print('*' * 50)\n",
    "print('Bag of n-grams vectorizer with Stop Words Removed')\n",
    "print('*' * 50)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=2000, stop_words='english')\n",
    "feature_vector = count_vectorizer.fit_transform(X)\n",
    "print(f'Shape of feature vector: {feature_vector.shape}')\n",
    "X_dense = feature_vector.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42)\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5a8803176ba603454a120d2af569b11c3f41e6e81f2a81589e795246afd0d6d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
